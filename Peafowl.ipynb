{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import os\n",
    "import h5py\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from __future__ import print_function\n",
    "#from keras.models import load_model\n",
    "metadata_type = np.dtype([(\"plaintext\", np.uint8, (16,)),\n",
    "                          (\"key\", np.uint8, (16,)),\n",
    "                          (\"ciphertext\", np.uint8, (16,))\n",
    "                          ])\n",
    "def check_file_exists(file_path):\n",
    "    if os.path.exists(file_path) == False:\n",
    "        print(\"Error: provided file path '%s' does not exist!\" % file_path)\n",
    "        sys.exit(-1)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_trs2h5(filename,outfile):\n",
    "    trsfile = open(filename,\"rb\")\n",
    "    j=0\n",
    "    hl=128\n",
    "    data_length=0\n",
    "    while j<hl:\n",
    "        data = trsfile.read(1)\n",
    "        tag = int(data.encode('hex'),16)\n",
    "        #assert tag in range(0x41,0x60)\n",
    "        if tag ==0x41:#number of traces\n",
    "            l=int(trsfile.read(1).encode('hex'),16) # length of value in byte\n",
    "            data = trsfile.read(l)\n",
    "            traces_num=int(data.encode('hex').decode('hex')[::-1].encode('hex_codec'),16)\n",
    "            print('trace_num'+ str(traces_num))\n",
    "\n",
    "        if tag ==0x42: #trace length\n",
    "            l=int(trsfile.read(1).encode('hex'),16)\n",
    "            data = trsfile.read(l)\n",
    "            trace_length = int(data.encode('hex').decode('hex')[::-1].encode('hex_codec'),16)\n",
    "            print('trace_length' + str(trace_length))\n",
    "\n",
    "        if tag==0x43: #sample coding\n",
    "            l=int(trsfile.read(1).encode('hex'),16)\n",
    "            assert l==0x01\n",
    "            data = trsfile.read(l)\n",
    "            samp_coding = int(data.encode('hex'),16)\n",
    "            assert samp_coding in [0x01,0x02,0x04,0x11,0x12,0x14]\n",
    "\n",
    "        if tag==0x44: #data length\n",
    "            l=int(trsfile.read(1).encode('hex'),16)\n",
    "            data = trsfile.read(l)\n",
    "            data_length = int(data.encode('hex').decode('hex')[::-1].encode('hex_codec'),16)\n",
    "            assert data_length ==32 or data_length == 48\n",
    "            print('data_length' + str(data_length))\n",
    "\n",
    "        if tag in range(0x45,0x5F):# ignored information\n",
    "            l=int(trsfile.read(1).encode('hex'),16)\n",
    "            data = trsfile.read(l)\n",
    "\n",
    "        if tag == 0x5F: #trace length\n",
    "            assert int(trsfile.read(1).encode('hex'),16) == 0x00\n",
    "\n",
    "            print('end of header')\n",
    "            break\n",
    "\n",
    "        j=j+1\n",
    "    h5f = h5py.File(outfile,'w')\n",
    "\n",
    "    metadata = []\n",
    "    traces = []\n",
    "    for i in range(traces_num):\n",
    "    #for i in range(100):\n",
    "\n",
    "        #print(i)\n",
    "        #process crypt data\n",
    "        key=np.zeros(shape=(16,),dtype=np.uint8)\n",
    "        plaintext=[]\n",
    "        ciphertext=[]\n",
    "        #the data are in order: plaintext, ciphertext, key.\n",
    "        #should be in accordence to h52trs\n",
    "        if data_length != 0:\n",
    "            for p in range(16):\n",
    "                plaintext.append(np.int8(int(trsfile.read(1).encode('hex'),16)))\n",
    "            for c in range(16):\n",
    "                ciphertext.append(np.int8(int(trsfile.read(1).encode('hex'),16)))\n",
    "            if data_length==48:\n",
    "                for k in range(16):\n",
    "                    key[k]=np.int8(int(trsfile.read(1).encode('hex'),16))\n",
    "        one_metadata=np.array((plaintext,key,ciphertext),dtype=metadata_type)\n",
    "        metadata.append(one_metadata)\n",
    "\n",
    "        #process trace points\n",
    "        one_trace_list=[]\n",
    "        for pnt in range(trace_length):\n",
    "            if (samp_coding==0x01):\n",
    "                one_trace_list.append(np.int8(int(trsfile.read(1).encode('hex'),16)))\n",
    "            if (samp_coding==0x14):\n",
    "                s1=trsfile.read(1)\n",
    "                s2=trsfile.read(1)\n",
    "                s3=trsfile.read(1)\n",
    "                s4=trsfile.read(1)\n",
    "                s=s4+s3+s2+s1\n",
    "                one_trace_list.append(np.float(struct.unpack('!f',s)[0]))\n",
    "        if (samp_coding==0x01):\n",
    "            one_trace=np.array(one_trace_list,dtype=np.int8)\n",
    "        if (samp_coding==0x14):\n",
    "            one_trace=np.array(one_trace_list,dtype=np.float)\n",
    "\n",
    "        traces.append(one_trace)\n",
    "\n",
    "    h5f.create_dataset('metadata',data = metadata)\n",
    "    h5f.create_dataset('traces',data = traces)\n",
    "    h5f.flush()\n",
    "    h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_file=''\n",
    "out_file=' '\n",
    "temp_trs2h5(in_file,out_file)\n",
    "f=h5py.File(out_file,'r')\n",
    "\n",
    "for key in ['metadata','traces']:\n",
    "    print(f[key].name)\n",
    "    print(f[key].shape)\n",
    "    #print(f[key].value)\n",
    "    print(f[key].dtype)\n",
    "t=f['traces']\n",
    "#for i in range(20):\n",
    "#    print(t[0][i])\n",
    "print('plaintext '+str(f['metadata'][0]['plaintext']))\n",
    "print('key' + str(f['metadata'][0]['key']))\n",
    "print('ciphertext '+str(f['metadata'][0]['ciphertext']))\n",
    "\n",
    "\n",
    "i=0\n",
    "for t in f['traces']:\n",
    "    if i < 10:\n",
    "        plt.title('trace')\n",
    "        #plt.xlabel('number of traces')\n",
    "        #plt.ylabel('rank')\n",
    "        plt.grid(True)\n",
    "        plt.plot(t)\n",
    "        i=i+1\n",
    "    else:\n",
    "        break\n",
    "\n",
    "plt.show()\n",
    "#plt.figure()\n",
    "\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_cls_ANOVA_key(traces,kbyte_index,key): \n",
    "    num_samp = len(traces[0])\n",
    "    num_trs = len(traces)\n",
    "    avr = [[0.0 for s in range(num_samp)] for i in range(256)]\n",
    "    avr=np.array(avr)\n",
    "    num_each_cls = [0 for i in range(256)]\n",
    "    point_index=[j for j in range(num_samp)]\n",
    "    for i in range(len(traces)):\n",
    "        #print(i,end=' ')\n",
    "        k = key[i][kbyte_index]\n",
    "        num_each_cls[k]=num_each_cls[k]+1\n",
    "        \n",
    "        avr[k]=avr[k] + traces[i][point_index]\n",
    "        \n",
    "    assert np.array(num_each_cls).sum()==num_trs\n",
    "    for i in range(256):\n",
    "        \n",
    "        avr[i]=avr[i]/num_each_cls[i]\n",
    "    \n",
    "    \n",
    "    avr_sum = np.array([0.0 for i in range(num_samp)])\n",
    "    square_sum = np.array([0.0 for i in range(num_samp)])\n",
    "    for i in range(num_samp):\n",
    "        avr_sum[i]=avr[:,i].sum()\n",
    "        square_sum[i]=(avr[:,i]**2).sum()\n",
    "    \n",
    "    var=square_sum/256-(avr_sum/256)**2\n",
    "    return (avr,var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h5file=h5py.File(out_file,'r')\n",
    "traces=h5file['traces']\n",
    "keys=h5file['metadata']['key']\n",
    "print(traces[0])\n",
    "print(keys[15])\n",
    "(avr,var)=temp_cls_ANOVA_key(traces,15,keys)\n",
    "\n",
    "for t in avr:        \n",
    "    plt.plot(t)\n",
    "plt.title('Average in each class')\n",
    "plt.grid(True)\n",
    "plt.rcParams['figure.figsize']=(20,8)   \n",
    "plt.show()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(var)\n",
    "plt.title('Variance of each class')\n",
    "plt.grid(True)\n",
    "plt.rcParams['figure.figsize']=(20,8)   \n",
    "plt.show()\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_points(var,threshold):\n",
    "    res = []\n",
    "    for i in range(len(var)):\n",
    "        if var[i]>threshold:\n",
    "            res.append(i)\n",
    "    return res\n",
    "def target_points_top(var,num):\n",
    "    res = []\n",
    "    var_list=list(var)\n",
    "    temp_list=list(var)\n",
    "    temp_list.sort(reverse=True)\n",
    "    \n",
    "    for i in range(num):\n",
    "        res.append(var_list.index(temp_list[i]))\n",
    "    res.sort()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar=target_points(var,0.5)\n",
    "print(len(tar))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_labelize(keys,keybyte_index):\n",
    "    return keys[:, keybyte_index]\n",
    "def temp_extract_traces(traces_file, labeled_traces_file,profiling_index,attack_index, target_points,keybyte_index):\n",
    "    #check_file_exists(traces_file)\n",
    "    #check_file_exists(os.path.dirname(labeled_traces_file))\n",
    "    # Open the raw traces HDF5 for reading\n",
    "    try:\n",
    "        in_file  = h5py.File(traces_file, \"r\")\n",
    "    except:\n",
    "        print(\"Error: can't open HDF5 file '%s' for reading (it might be malformed) ...\" % traces_file)\n",
    "        sys.exit(-1)\n",
    "        \n",
    "    # Open the output labeled file for writing\n",
    "    try:\n",
    "        out_file = h5py.File(labeled_traces_file, \"w\")\n",
    "    except:\n",
    "        print(\"Error: can't open HDF5 file '%s' for writing ...\" % labeled_traces_file)\n",
    "        sys.exit(-1)\n",
    "        \n",
    "    raw_traces = in_file['traces']\n",
    "    raw_plaintexts = in_file['metadata']['plaintext']\n",
    "    raw_ciphertexts = in_file['metadata']['ciphertext']\n",
    "    raw_keys = in_file['metadata']['key']\n",
    "    \n",
    "    raw_traces_profiling = np.zeros([len(profiling_index), len(target_points)], raw_traces.dtype)\n",
    "    current_trace=0\n",
    "    \n",
    "    for t in profiling_index:       \n",
    "        print(t,end=' ')\n",
    "        \n",
    "        raw_traces_profiling[current_trace]=raw_traces[t,target_points]\n",
    "            \n",
    "        current_trace = current_trace+1    \n",
    "    \n",
    "    print(raw_traces_profiling.shape)\n",
    "    \n",
    "    raw_traces_attack = np.zeros([len(attack_index), len(target_points)], raw_traces.dtype)\n",
    "    current_trace=0\n",
    "    for t in attack_index:       \n",
    "        \n",
    "        raw_traces_attack[current_trace]=raw_traces[t,target_points]\n",
    "        current_trace = current_trace+1    \n",
    "    \n",
    "    print(raw_traces_attack.shape)\n",
    "    \n",
    "    \n",
    "    # Compute our labels\n",
    "    labels_profiling = temp_labelize(raw_keys[profiling_index], keybyte_index)\n",
    "    labels_attack  = temp_labelize(raw_keys[attack_index], keybyte_index)\n",
    "    \n",
    "    # Create our HDF5 hierarchy in the output file:\n",
    "    # \t- Profilinging traces with their labels\n",
    "    #\t- Attack traces with their labels\n",
    "    profiling_traces_group = out_file.create_group(\"Profiling_traces\")\n",
    "    attack_traces_group = out_file.create_group(\"Attack_traces\")\n",
    "    # Datasets in the groups\n",
    "    profiling_traces_group.create_dataset(name=\"traces\", data=raw_traces_profiling, \n",
    "                                          dtype=raw_traces_profiling.dtype)\n",
    "    attack_traces_group.create_dataset(name=\"traces\", data=raw_traces_attack, dtype=raw_traces_attack.dtype)\n",
    "    # Labels in the groups\n",
    "    profiling_traces_group.create_dataset(name=\"labels\", data=labels_profiling, dtype=labels_profiling.dtype)\n",
    "    attack_traces_group.create_dataset(name=\"labels\", data=labels_attack, dtype=labels_attack.dtype)\n",
    "    # Put the metadata (plaintexts, keys, ...) so that one can check the key rank\n",
    "   \n",
    "    profiling_metadata = np.array([(raw_plaintexts[n], raw_keys[n], raw_ciphertexts[n]) for n in profiling_index], \n",
    "                                  dtype=metadata_type)\n",
    "    profiling_traces_group.create_dataset(\"metadata\", data=profiling_metadata, dtype=metadata_type)\n",
    "    attack_metadata = np.array([(raw_plaintexts[n], raw_keys[n], raw_ciphertexts[n]) for n in attack_index], \n",
    "                               dtype=metadata_type)\n",
    "    attack_traces_group.create_dataset(\"metadata\", data=attack_metadata, dtype=metadata_type)\n",
    "\n",
    "    out_file.flush()\n",
    "    out_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_out_file=' '\n",
    "\n",
    "\n",
    "temp_extract_traces(out_file,labeled_out_file,[i for i in range(249505)],[i for i in range(259505-5000,259504)],tar,12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import core, Flatten, Dense, Input, Dropout, Conv1D, MaxPooling1D, GlobalAveragePooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model\n",
    "\n",
    "def load_data(file, load_metadata=False):\n",
    "    #check_file_exists(file)\n",
    "    # Open the database HDF5 for reading\n",
    "    try:\n",
    "        in_file  = h5py.File(file, \"r\")\n",
    "    except:\n",
    "        print(\"Error: can't open HDF5 file '%s' for reading (it might be malformed) ...\" % file)\n",
    "        sys.exit(-1)\n",
    "    # Load profiling traces\n",
    "    X_profiling = np.array(in_file['Profiling_traces/traces'], dtype=np.int8)\n",
    "    # Load profiling labels\n",
    "    Y_profiling = np.array(in_file['Profiling_traces/labels'])\n",
    "    # Load attacking traces\n",
    "    X_attack = np.array(in_file['Attack_traces/traces'], dtype=np.int8)\n",
    "    # Load attacking labels\n",
    "    Y_attack = np.array(in_file['Attack_traces/labels'])\n",
    "    if load_metadata == False:\n",
    "        return (X_profiling, Y_profiling), (X_attack, Y_attack)\n",
    "    else:\n",
    "        return (X_profiling, Y_profiling), (X_attack, Y_attack), (in_file['Profiling_traces/metadata'], \n",
    "                                                                  in_file['Attack_traces/metadata'])\n",
    "\n",
    "    #### Training high level function\n",
    "def train_model(X_profiling, Y_profiling, model, save_file_name,set_validation_split=0.05,\n",
    "                set_batch_size=128, set_verbose = 1, set_epochs=30):\n",
    "   # check_file_exists(os.path.dirname(save_file_name))\n",
    "    # Save model every epoch\n",
    "    save_model = ModelCheckpoint(save_file_name)\n",
    "    callbacks=[save_model]\n",
    "    # Get the input layer shape\n",
    "    input_layer_shape = model.get_layer(index=0).input_shape\n",
    "    # Sanity check\n",
    "    if input_layer_shape[1] != len(X_profiling[0]):\n",
    "        print(\"Error: model input shape %d instead of %d is not expected ...\" % (input_layer_shape[1],\n",
    "                                                                                 len(X_profiling[0])))\n",
    "        sys.exit(-1)\n",
    "    # Adapt the data shape according our model input\n",
    "    if len(input_layer_shape) == 2:\n",
    "        # This is a MLP\n",
    "        Reshaped_X_profiling = X_profiling\n",
    "    elif len(input_layer_shape) == 3:\n",
    "        # This is a CNN: expand the dimensions\n",
    "        Reshaped_X_profiling = X_profiling.reshape((X_profiling.shape[0], X_profiling.shape[1], 1))\n",
    "    else:\n",
    "        print(\"Error: model input shape length %d is not expected ...\" % len(input_layer_shape))\n",
    "        sys.exit(-1)\n",
    "\n",
    "    history = model.fit(x=Reshaped_X_profiling, y=to_categorical(Y_profiling, num_classes=256),validation_split=set_validation_split,\n",
    "                        batch_size=set_batch_size, verbose=set_verbose, epochs=set_epochs, callbacks=callbacks)\n",
    "    \n",
    "    return history\n",
    "def plot(history,width=18,height=6):\n",
    "    plt.rcParams['figure.figsize']=(width,height)   \n",
    "    # summarize history for accuracy\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    #plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labeled_out_file='SM4KeyLoading_DeepLearning_labeled.h5'\n",
    "(X_profiling, Y_profiling), (X_attack, Y_attack), (Profiling_metadata, Attack_metadata) = load_data(labeled_out_file,True)\n",
    "\n",
    "#### MLP  model\n",
    "def mlp(set_input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=set_input_dim, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(256, activation='softmax'))\n",
    "    optimizer = RMSprop(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "mlp_model = mlp(len(X_profiling[0]))\n",
    "\n",
    "###CNN model\n",
    "def cnn(trace_length):\n",
    "    input_shape = (trace_length,1)\n",
    "    trace_input = Input(shape=input_shape)\n",
    "    \n",
    "    #Block1\n",
    "    x = Conv1D(8,kernel_size=10, activation='relu',padding='same',name='block1_conv1')(trace_input)\n",
    "    x = MaxPooling1D(2, strides=2, name='block1_pool')(x)\n",
    "    \n",
    "    #Block2\n",
    "    x = Conv1D(32, 1, activation = 'relu', padding='same',name='block2_conv1')(x)\n",
    "    x = MaxPooling1D(2, strides=2, name='block2_pool')(x)\n",
    "    \n",
    "    #if drop_out:\n",
    "    #    x = Dropout(drop_out)(x)\n",
    "    # Classification block\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    \n",
    "    #x = Dense(512, activation='relu', name='fc1')(x)\n",
    "    x = Dense(256, activation='softmax', name='predictions')(x)\n",
    "    \n",
    "    inputs = trace_input\n",
    "    model = Model(inputs, x,name='cnn')\n",
    "    optimizer = RMSprop(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "cnn_model = cnn(len(X_profiling[0]))\n",
    "\n",
    "trained_model='SM4KeyLoading_DeepLearning_model.h5'\n",
    "history=train_model(X_profiling, Y_profiling, mlp_model, trained_model,set_validation_split=0.2,set_batch_size=512,\n",
    "                    set_verbose=1,set_epochs=30)\n",
    "plot(history,12,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the prob of the key candidates for a give set of predictions\n",
    "def prob_sum(predictions, min_trace_idx, max_trace_idx, last_key_bytes_proba):\n",
    "    # Compute the rank\n",
    "    if len(last_key_bytes_proba) == 0:\n",
    "        # If this is the first rank we compute, initialize all the estimates to zero\n",
    "        key_bytes_proba = np.zeros(256)\n",
    "    else:\n",
    "        # This is not the first rank we compute: we optimize things by using the\n",
    "        # previous computations to save time!\n",
    "        key_bytes_proba = last_key_bytes_proba\n",
    "\n",
    "    for p in range(0, max_trace_idx-min_trace_idx):\n",
    "        # Go back from the class to the key byte. '2' is the index of the byte (third byte) of interest.\n",
    "        #plaintext = metadata[min_trace_idx + p]['plaintext'][S_index]\n",
    "        for i in range(0, 256):\n",
    "            # Our candidate key byte probability is the sum of the predictions logs\n",
    "            proba = predictions[p][i]\n",
    "            \n",
    "            key_bytes_proba[i] += proba\n",
    "        \n",
    "       \n",
    "    return key_bytes_proba\n",
    "def full_probs(model_file, dataset, min_trace_idx, max_trace_idx, rank_step):\n",
    "    check_file_exists(model_file)\n",
    "    try:\n",
    "        model = load_model(model_file)\n",
    "    except:\n",
    "        print(\"Error: can't load Keras model file '%s'\" % model_file)\n",
    "        sys.exit(-1)\n",
    "    # Check for overflow\n",
    "    if max_trace_idx > dataset.shape[0]:\n",
    "        print(\"Error: asked trace index %d overflows the total traces number %d\" % (max_trace_idx, dataset.shape[0]))\n",
    "        sys.exit(-1)\n",
    "    # Get the input layer shape\n",
    "    input_layer_shape = model.get_layer(index=0).input_shape\n",
    "    # Sanity check\n",
    "    if input_layer_shape[1] != len(dataset[0, :]):\n",
    "        print(\"Error: model input shape %d instead of %d is not expected ...\" % (input_layer_shape[1], len(dataset[0, :])))\n",
    "        sys.exit(-1)\n",
    "    # Adapt the data shape according our model input\n",
    "    if len(input_layer_shape) == 2:\n",
    "        # This is a MLP\n",
    "        input_data = dataset[min_trace_idx:max_trace_idx, :]\n",
    "    elif len(input_layer_shape) == 3:\n",
    "        # This is a CNN: reshape the data\n",
    "        input_data = dataset[min_trace_idx:max_trace_idx, :]\n",
    "        input_data = input_data.reshape((input_data.shape[0], input_data.shape[1], 1))\n",
    "    else:\n",
    "        print(\"Error: model input shape length %d is not expected ...\" % len(input_layer_shape))\n",
    "        sys.exit(-1)\n",
    "\n",
    "    # Predict our probabilities\n",
    "    predictions = model.predict(input_data)\n",
    "\n",
    "    index = np.arange(min_trace_idx+rank_step, max_trace_idx, rank_step)\n",
    "    f_probs = np.zeros((256, len(index)), dtype=np.float16)\n",
    "    key_bytes_proba = []\n",
    "    for t, i in zip(index, range(0, len(index))):\n",
    "        key_bytes_proba = prob_sum(predictions[t-rank_step:t], t-rank_step, t, key_bytes_proba)\n",
    "        num_pred_trs = (i+1)*rank_step\n",
    "        #print(num_pred_trs)\n",
    "        f_probs[:,i]=key_bytes_proba/num_pred_trs\n",
    "    plt.rcParams['figure.figsize']=(12,5) \n",
    "    for i in range(256):    \n",
    "        plt.plot(index,f_probs[i])\n",
    "        #print(f_probs[i])\n",
    "    plt.show()\n",
    "    pred_key=np.where(key_bytes_proba==max(key_bytes_proba))[0][0]\n",
    "    \n",
    "    #pred_key=np.where(key_bytes_proba==(key_bytes_proba))[0][0]\n",
    "    #print(hex(pred_key))\n",
    "    return (pred_key,key_bytes_proba)\n",
    "def pred_top(key_bytes_proba,n):\n",
    "    res=[]\n",
    "    sorted_key_bytes_proba=sorted(key_bytes_proba,reverse=True)\n",
    "    for i in range(n):\n",
    "        temp=np.where(key_bytes_proba==sorted_key_bytes_proba[i])\n",
    "        assert len(temp[0])==1\n",
    "        res.append([temp[0][0],sorted_key_bytes_proba[i]])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_out_file='SM4KeyLoading_DeepLearning_labeled.h5'\n",
    "(X_profiling, Y_profiling), (X_attack, Y_attack), (Profiling_metadata, Attack_metadata) = load_data(labeled_out_file,True)\n",
    "\n",
    "trained_model='SM4KeyLoading_DeepLearning_model.h5'\n",
    "(f_probs,key_bytes_proba)=full_probs(trained_model,X_attack,0,4999,1)\n",
    "print(hex(f_probs))\n",
    "\n",
    "print(key_bytes_proba)\n",
    "for i in pred_top(key_bytes_proba,256):\n",
    "    print(hex(i))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
